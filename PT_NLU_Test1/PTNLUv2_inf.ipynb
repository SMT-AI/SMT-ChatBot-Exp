{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "438fc3e0-7a46-4351-bb41-731edd887ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import logging\n",
    "from typing import Dict, List, Tuple, Any\n",
    "import mlflow\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0716a24e-8f18-4f9a-8000-0d585fd58141",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('erp_bot.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "537d5e77-517d-4825-a133-f17ac8f89c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBv2Dataset(Dataset):\n",
    "    def __init__(self, texts: List[str], main_intents: List[str], sub_intents: List[str], tokenizer: Any, max_len: int = 128):\n",
    "        self.texts = texts\n",
    "        self.main_intents = main_intents\n",
    "        self.sub_intents = sub_intents\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        main_intent = self.main_intents[idx]\n",
    "        sub_intent = self.sub_intents[idx]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text, \n",
    "            add_special_tokens = True,\n",
    "            max_length = self.max_len,\n",
    "            padding = 'max_length',\n",
    "            truncation = True,\n",
    "            return_attention_mask = True,\n",
    "            return_tensors = 'pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'text' : text,\n",
    "            'input_ids' : encoding['input_ids'].flatten(),\n",
    "            'attention_mask' : encoding['attention_mask'].flatten(),\n",
    "            'main_intent' : torch.tensor(main_intent, dtype = torch.long),\n",
    "            'sub_intent' : torch.tensor(sub_intent, dtype = torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6227e103-4e50-46fb-ace0-19164a8aefa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HierarchicalIntentClassifier(nn.Module):\n",
    "    def __init__(self, intent_structure: Dict[str, Dict[str, List[str]]]):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.drop = nn.Dropout(0.3)\n",
    "        \n",
    "        # Main intent classifier\n",
    "        self.n_main_intents = len(intent_structure)\n",
    "        self.main_classifier = nn.Linear(768, self.n_main_intents)\n",
    "        \n",
    "        # Create sub-classifiers for each main intent\n",
    "        self.sub_classifiers = nn.ModuleDict({\n",
    "            main_intent: nn.Linear(768, len(sub_intents))\n",
    "            for main_intent, sub_intents in intent_structure.items()\n",
    "        })\n",
    "        \n",
    "        # Store the structure for reference\n",
    "        self.intent_structure = intent_structure\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, main_intent=None):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs[1]\n",
    "        dropped = self.drop(pooled_output)\n",
    "        \n",
    "        # Get main intent prediction\n",
    "        main_output = self.main_classifier(dropped)\n",
    "        \n",
    "        if main_intent is None:\n",
    "            # During prediction, use predicted main intent\n",
    "            main_intent = torch.argmax(main_output, dim=1)\n",
    "        \n",
    "        # Get sub-intent prediction for the specific main intent\n",
    "        batch_size = input_ids.size(0)\n",
    "        sub_outputs = torch.zeros(batch_size, max(len(subs) for subs in self.intent_structure.values()))\n",
    "        sub_outputs = sub_outputs.to(input_ids.device)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            intent_name = list(self.intent_structure.keys())[main_intent[i]]\n",
    "            sub_classifier = self.sub_classifiers[intent_name]\n",
    "            sub_output = sub_classifier(dropped[i].unsqueeze(0))\n",
    "            # Pad if necessary\n",
    "            sub_outputs[i, :sub_output.size(1)] = sub_output\n",
    "            \n",
    "        return main_output, sub_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0656ea99-adf3-4037-95f4-abc582bc8776",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntentClassifier:\n",
    "    def __init__(self, model_path: str = None, confidence_threshold: float = 0.7):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.model = None\n",
    "        self.intent_structure = {}\n",
    "        self.main_intent_to_id = {}\n",
    "        self.sub_intent_to_id = {}\n",
    "        self.confidence_threshold = confidence_threshold\n",
    "        \n",
    "        if model_path:\n",
    "            self.load_model(model_path)\n",
    "\n",
    "    def load_model(self, path: str):\n",
    "        checkpoint = torch.load(path, map_location=self.device)\n",
    "        self.intent_structure = checkpoint['intent_structure']\n",
    "        self.model = HierarchicalIntentClassifier(self.intent_structure)\n",
    "        self.model.load_state_dict(checkpoint['model_state'])\n",
    "        self.main_intent_to_id = checkpoint['main_intent_to_id']\n",
    "        self.sub_intent_to_id = checkpoint['sub_intent_to_id']\n",
    "        self.model.to(self.device)\n",
    "        logger.info(f\"Model loaded from {path}\")\n",
    "\n",
    "    def predict(self, text: str) -> Tuple[str, str, float, float]:\n",
    "        self.model.eval()\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=128,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        input_ids = encoding['input_ids'].to(self.device)\n",
    "        attention_mask = encoding['attention_mask'].to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            main_outputs, _ = self.model(input_ids, attention_mask)\n",
    "            main_probs = torch.softmax(main_outputs, dim=1)\n",
    "            main_confidence, main_predicted = torch.max(main_probs, 1)\n",
    "            \n",
    "            # Check confidence threshold\n",
    "            if main_confidence.item() < self.confidence_threshold:\n",
    "                return (\n",
    "                    \"out_of_context_contact_llm\",\n",
    "                    \"handle_unknown_query\",\n",
    "                    main_confidence.item(),\n",
    "                    0.0\n",
    "                )\n",
    "            \n",
    "            # If confidence is good, proceed with normal prediction\n",
    "            main_id_to_intent = {v: k for k, v in self.main_intent_to_id.items()}\n",
    "            predicted_main_intent = main_id_to_intent[main_predicted.item()]\n",
    "            \n",
    "            # Get sub-intent prediction\n",
    "            dropped = self.model.drop(self.model.bert(input_ids, attention_mask)[1])\n",
    "            sub_classifier = self.model.sub_classifiers[predicted_main_intent]\n",
    "            sub_outputs = sub_classifier(dropped)\n",
    "            sub_probs = torch.softmax(sub_outputs, dim=1)\n",
    "            sub_confidence, sub_predicted = torch.max(sub_probs, 1)\n",
    "            \n",
    "            sub_id_to_intent = {v: k for k, v in self.sub_intent_to_id[predicted_main_intent].items()}\n",
    "            predicted_sub_intent = sub_id_to_intent[sub_predicted.item()]\n",
    "        \n",
    "        return (\n",
    "            predicted_main_intent,\n",
    "            predicted_sub_intent,\n",
    "            main_confidence.item(),\n",
    "            sub_confidence.item()\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "798d2247-b212-45b0-a42b-7ed6df05dd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_user_query(text: str, classifier: IntentClassifier):\n",
    "    main_intent, sub_intent, main_conf, sub_conf = classifier.predict(text)\n",
    "    \n",
    "    if main_intent == \"out_of_context_contact_llm\":\n",
    "        logger.info(f\"Query '{text}' detected as out-of-context (confidence: {main_conf:.2f})\")\n",
    "        logger.info(\"Forwarding to LLM microservice\")\n",
    "        return {\n",
    "            'status': 'llm_fallback',\n",
    "            'confidence': main_conf,\n",
    "            'original_query': text,\n",
    "            #Add your LLM microservice call here\n",
    "            'action': 'forward_to_llm'\n",
    "        }\n",
    "    else:\n",
    "        logger.info(f\"Query '{text}' matched intent: {main_intent}/{sub_intent}\")\n",
    "        logger.info(f\"Confidence scores - Main: {main_conf:.2f}, Sub: {sub_conf:.2f}\")\n",
    "        return {\n",
    "            'status': 'intent_matched',\n",
    "            'main_intent': main_intent,\n",
    "            'sub_intent': sub_intent,\n",
    "            'confidence': {\n",
    "                'main': main_conf,\n",
    "                'sub': sub_conf\n",
    "            }\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "880c91e5-7e08-499b-ab0a-9c63f63129c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-09 10:38:20,065 - __main__ - INFO - Model loaded from models/optimized_model.pt\n"
     ]
    }
   ],
   "source": [
    "classifier = IntentClassifier(confidence_threshold=0.8)\n",
    "classifier.load_model('models/optimized_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2f0a692e-f4cc-4e94-961a-521c7f05f66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_messages = [\n",
    "    \"How many working days are there in a week?\", \n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "215a14f1-549d-467f-8166-23011aaa9e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-09 10:39:49,898 - __main__ - INFO - Query 'How many working days are there in a week?' matched intent: employee_module/shift_master\n",
      "2025-05-09 10:39:49,898 - __main__ - INFO - Confidence scores - Main: 0.94, Sub: 0.25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: How many working days are there in a week?\n",
      "Result: {'status': 'intent_matched', 'main_intent': 'employee_module', 'sub_intent': 'shift_master', 'confidence': {'main': 0.9369529485702515, 'sub': 0.24527935683727264}}\n"
     ]
    }
   ],
   "source": [
    "for message in test_messages:\n",
    "    result = handle_user_query(message, classifier)\n",
    "    print(f\"\\nQuery: {message}\")\n",
    "    print(f\"Result: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502998fb-d276-43aa-9687-8c750b877488",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
