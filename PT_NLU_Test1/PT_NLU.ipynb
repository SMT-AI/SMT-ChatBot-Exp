{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c5c9ce6-a678-45b1-817f-a88831313347",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import json\n",
    "import logging\n",
    "from typing import Dict, List, Tuple\n",
    "import mlflow\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8149ca0-634e-4835-aeb9-7da0fe4e612f",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('erp_bot.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc64a9e0-dc7a-41d0-910f-bcc355660a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ERPDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'text': text,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c552112f-fe92-40ba-abbd-ea3cfa3ce501",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ERPIntentClassifier(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super.__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.drop = nn.Dropout(0.3)\n",
    "        self.fc = nn.Linear(768, n_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(\n",
    "            input_ids = input_ids,\n",
    "            attention_mask = attention_mask\n",
    "        )\n",
    "        pooled_output = outputs[1]\n",
    "        output = self.drop(pooled_output)\n",
    "        return self.fc(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b99a7af-cec8-44fb-957e-1b09ec307d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntentClassifier:\n",
    "    def __init__(self, model_path: str = None):\n",
    "        self.device = torch.device('cpu')\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.model_path = None\n",
    "        self.label_to_id = {}\n",
    "        if model_path:\n",
    "            self.load_model(model_path)\n",
    "\n",
    "    def load_model(self, path: str):\n",
    "        checkpoint = torch.load(path, map_location = self.device)\n",
    "        self.model = ERPIntentClassifier(len(checkpoint['label_to_id']))\n",
    "        self.model.load_state_dict(checkpoint['model_state'])\n",
    "        self.label_to_id = checkpoint['label_to_id']\n",
    "        self.model.to(self.device)\n",
    "        logger.info(f\"Model loader from {path}\")\n",
    "\n",
    "    def save_model(self, path: str):\n",
    "        torch.save({\n",
    "            'model_state': self.model.state_dict(),\n",
    "            'label_to_id': self.label_to_id\n",
    "        }, path) \n",
    "        logger.info(f\"Model saved to {path}\")\n",
    "\n",
    "    def train(self, train_data: Dict[str, List[str]], epochs = 5, batch_size: int = 16):\n",
    "        texts, labels = [], []\n",
    "        for intent, examples in train_data.items():\n",
    "            if intent not in self.label_to_id:\n",
    "                self.label_to_id[intent] = len(self.label_to_id)\n",
    "            text.extend(examples)\n",
    "            labels.extend([self.label_to_id[intent]]*len(examples))\n",
    "    \n",
    "        dataset = ERPDataset(texts, labels, self.tokenizer)\n",
    "        dataloader = DataLoader(dataset, batch_size = batch_size, shuffle = True)\n",
    "        self.model = ERPIntentClassifier(len(self.label_to_id))\n",
    "        self.model.to(self.device)\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr = 2e-5)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        mlflow.start_run()\n",
    "        for epoch in range(epochs):\n",
    "            self.model.train()\n",
    "            total_loss = 0\n",
    "            for batch in dataloader:\n",
    "                optimizer.zero_grad()\n",
    "                input_ids = batch['input_ids'].to(self.device)\n",
    "                attention_mask = batch['attention_mask'].to(self.device)\n",
    "                labels = batch['label'].to(self.device)\n",
    "                outputs = self.model(input_ids, attention_mask)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "            avg_loss = total_loss / len(dataloader)\n",
    "            logger.info(f\"Epoch: {epoch + 1} / {epochs}, Loss: {avg_loss:.4f}\")\n",
    "            mlflow.log_metric(\"loss\", avg_loss, step = epoch)\n",
    "\n",
    "        mlflow.end_run()\n",
    "\n",
    "    def predict(self, text: str) -> Tuple [str, float]:\n",
    "        self.model.eval()\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens = True,\n",
    "            max_length = 128,\n",
    "            padding = 'max_length',\n",
    "            truncation = True,\n",
    "            return_attention_mask = True,\n",
    "            return_tensors = 'pt'\n",
    "        )\n",
    "\n",
    "        input_ids = encoding['input_ids'].to(self.device)\n",
    "        attention_mask = encoding['attention_mask'].to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = seld.model(input_ids, attention_mask)\n",
    "            probabilities = torch.softmax(outputs, dim = 1)\n",
    "            confidence, predicted = torch.max(probabilities, 1)\n",
    "\n",
    "        id_to_\n",
    "        \n",
    "                \n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
